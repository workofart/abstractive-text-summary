# Abstractive Text Summarization

Authors: Hanxiang (Henry) Pan, Xinyu Ma

We approached this problem by using a baseline model that's composed of a gated recurrent autoencoder with attention mechanism.

We analyzed it's performance and pitfalls and implemented the transformer architecture with multi-headed (self) attention mechanism.


## Code

The baseline model can be found in `Project_Baseline.ipynb`.
The improved model can be found in `Project_Improved.ipynb`.

## Report

All the analyses and discussions can be found in `Final_Project.pdf`.


## License

MIT license. 
